Machine Translation Report: Low-Resource Southern Uzbek

1. Executive Summary

This work studies machine translation for Southern Uzbek (Arabic script, uzs_Arab) into English and Standard Uzbek. Zero-shot experiments with large multilingual models (NLLB-200 and M2M100) show that direct translation quality is low due to script and dialect mismatch.

By redesigning the inference pipeline using pivot translation, beam search, ensembling, and post-processing, translation quality can be substantially improved without any fine-tuning.

The best system (NLLB-200 with pivot translation) achieves:
BLEU 24.46, chrF 34.92, BERTScore F1 64.16 for Southern Uzbek to English.
BLEU 14.75, chrF 43.84, METEOR 16.04, BERTScore F1 86.6 for Southern Uzbek to Standard Uzbek. (Normalization)

This represents almost a three times improvement in BLEU over the direct zero-shot baseline.

2. Dataset and Preprocessing

We use the Lutfiy Southern Uzbek parallel corpus containing approximately 2,500 sentence pairs.

Source: Southern Uzbek (Arabic script, uzs_Arab)
Targets:

* English (eng_Latn)
* Standard Uzbek (uzn_Latn)

Preprocessing includes data cleaning and duplicate removal, synthetic reference generation for normalization, and an 80/20 train-validation split.

Additional improvements include regex-based text normalization and automatic post-processing to remove repeated n-grams and decoding artifacts.

3. Models and Methodology

Models:

* NLLB-200 (Distilled 600M), which explicitly supports Arabic-script Uzbek.
* M2M100 (418M), used as a baseline and performing poorly on Arabic-script input.

Decoding strategies.

Baseline:

* Greedy zero-shot decoding.

Improved pipeline:

* Beam search with beam size 5.
* Pivot translation: Southern Uzbek to Standard Uzbek to English.
* Ensembling of multiple candidates with heuristic selection.
* Post-processing to remove repetition and artifacts.

4. Evaluation Results

Southern Uzbek to English:

NLLB Direct:
BLEU 8.80, chrF 38.27, METEOR 27.10, BERTScore F1 60.29

NLLB Pivot:
BLEU 24.46, chrF 34.92, METEOR 28.74, BERTScore F1 64.16

NLLB Ensemble:
BLEU 24.33, chrF 39.82, METEOR 28.94, BERTScore F1 64.09

M2M100:
BLEU 10.28, chrF 18.59, METEOR 16.80, BERTScore F1 46.40

Southern Uzbek to Standard Uzbek (Normalization):

NLLB:
BLEU 14.75, chrF 43.84, METEOR 16.04, BERTScore F1 86.65

M2M100:
BLEU 2.71, chrF 10.01, METEOR 3.04, BERTScore F1 74.03

5. Discussion

Models that do not explicitly support Arabic-script Uzbek suffer from a strong script barrier, which explains the extremely poor performance of M2M100.

Dialect normalization using NLLB partially converts Southern Uzbek into Standard Uzbek. The very high BERTScore of 86.65 indicates strong semantic preservation despite low BLEU.

Pivot translation using Standard Uzbek as an intermediate representation dramatically improves English translation quality, increasing BLEU from 8.8 to 24.5.

BLEU is very strict for morphologically rich languages, while chrF better reflects character-level similarity. BERTScore shows that even low-BLEU systems may preserve meaning reasonably well.

Overall, the improved pipeline yields almost a three times BLEU improvement over the direct baseline.

6. Limitations

Synthetic references for normalization introduce evaluation bias. Some hallucinations and repetition artifacts remain. No fine-tuning was performed, and task-specific training would likely further improve results.

7. Conclusion

This work shows that practically usable translation quality for a low-resource language can be achieved without fine-tuning by carefully designing the inference pipeline.

Pivot translation, beam search, ensembling, and post-processing transform a weak zero-shot baseline into a significantly stronger system. Among the tested models, NLLB-200 with pivot translation is clearly the best choice for Southern Uzbek translation.