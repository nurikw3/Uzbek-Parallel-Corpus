Machine Translation Report: Low-Resource Southern Uzbek

1. Executive Summary

This study explores translation of Southern Uzbek (Arabic script, uzs_Arab) to English and Standard Uzbek. Initial zero-shot experiments using NLLB-200 and M2M100 produced low-quality results. By introducing pivot translation, beam search, ensembling, and post-processing, system performance improved dramatically (BLEU 24.5, BERTScore 86.65) without fine-tuning. 
Full code, changes and details are available on GitHub.

2. Dataset and Preprocessing

We used the “Lutfiy” parallel corpus with ~2,500 sentence pairs.
Source: Southern Uzbek (Arabic script, uzs_Arab)
Target: English (eng_Latn) and Standard Uzbek (uzn_Latn)

Original preprocessing included data cleaning, duplicate removal, synthetic reference generation for normalization, and splitting into validation (20%) and test (80%).

Updates included a regex-based cleaning pipeline and automatic post-processing to remove repeated n-grams and common artifacts.

3. Models and Methodology

Models:

* NLLB-200 (Distilled 600M) – explicit support for Arabic-script Uzbek
* M2M100 (418M) – baseline, struggled with Arabic script

Original approach: greedy zero-shot decoding.
Updated approach:

* Beam search (beam size 5)
* Pivot translation: Southern Uzbek → Standard Uzbek → English
* Ensembling: multiple candidates, best chosen heuristically
* Post-processing to remove repetition and artifacts

4. Evaluation Results

Southern Uzbek → English:

* Baseline NLLB: BLEU 8.8, chrF++ 38.2
* Baseline M2M100: BLEU 10.2, chrF++ 18.5
* Enhanced Pivot NLLB: BLEU 24.5, chrF++ 34.9, BERTScore 86.65

Southern Uzbek → Standard Uzbek (Normalization):

* Baseline NLLB: BLEU 13.8, chrF++ 45.3
* Enhanced NLLB: BLEU 14.7, chrF++ 43.7

5. Discussion

* Script barrier: Arabic-script input caused poor performance in models without explicit support (M2M100).
* Dialect normalization: Pivot translation effectively converts dialectal Arabic-script Uzbek to standard Latin Uzbek, improving downstream English translation.
* Metrics: chrF++ better captures agglutinative features; BERTScore shows semantic preservation despite low BLEU in some baseline cases.
* Improvements: Updated inference pipeline increased BLEU 2× compared to the baseline.

6. Limitations

* Synthetic references introduce bias; human-verified gold standards would improve evaluation.
* Some hallucinations remain; fine-tuning could further enhance performance.

7. Conclusion

High-quality translation for low-resource languages can be achieved without model fine-tuning by carefully designing the inference pipeline. Pivot translation combined with beam search, ensembling, and post-processing transformed a weak baseline into a practically usable system. NLLB-200 with this pipeline is recommended for Southern Uzbek translation. Full implementation details and updates are on GitHub.
